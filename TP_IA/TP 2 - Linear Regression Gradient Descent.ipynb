{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7844b296",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Practical lesson 2 <br/> Sébastien Harispe <br/> IMT Mines Alès </div>\n",
    "\n",
    "<h1><center>Practical Machine Learning: <br/> Supervised Machine Learning <br/> Linear Regression & Gradient Descent</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d60fc",
   "metadata": {},
   "source": [
    "# Aim of the practical lesson\n",
    "\n",
    "In this practical lesson we will:\n",
    "* Define a playground to introduce practical aspects of Supervised Machine Learning.\n",
    "* Focus on Linear Regression\n",
    "* Develop our first supervised Machine Learning algorithm.\n",
    "\n",
    "We will mainly use `numpy` and `matplotlib` Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d0693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234) # This is to make our work reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce552f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  We will first create a function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ that will represent the underlying function we would like to approximate, i.e. $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, <br/>\n",
    "    e.g. $f(x) = -2 + 1.8x - 0.1x^2 - 0.18x^3 + 0.03x^4$. <br/>\n",
    "<br/>\n",
    " Define a Python function corresponding to $f$.\n",
    " <br/>\n",
    "    \n",
    "<b>Important</b>: Keep in mind that this function will never be known when you'll deal with a Machine learning problem. \n",
    "</div>\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41813e1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  # YOUR TURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5378ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff01d69",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  Set a range of values [min_x, max_x] of  $\\mathcal{X}$ on which you will restrict your analysis. <br/>\n",
    "   This will be required for plotting our results. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN Modify this according to the function you choosed\n",
    "min_x = -4\n",
    "max_x = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1eb971",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  Plot the function using matplolib. <br/>\n",
    "  Use np.linspace to generate the x sampling\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de6a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR TURN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26935f81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  Generate samples in the predifined range of values [min_x, max_x] and compute the expected target value (label) for each sample. <br/>\n",
    "  We will assume that we are not able to observe exact values of $f$. To do so we will consider that for any $x \\in \\mathcal{X}$ we only have access to a label $f(x) + \\epsilon$.\n",
    "    Assume that $\\epsilon$ is sampled from a standard normal distribution, i.e. univariate normal (Gaussian) distribution of mean 0 and variance 1. <br/>\n",
    "    You will also assume that sampling $\\mathcal{X}$ is made considering a uniform distribution. \n",
    "    <br/> <br/>\n",
    "    Consider that you have nb_samples labelled observations in your dataset, i.e. pairs $(x,y)$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef41642",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_samples = 500\n",
    "\n",
    "x = # YOUR TURN\n",
    "y = # YOUR TURN\n",
    "\n",
    "print(f\"x : {x.shape}\")\n",
    "print(f\"y : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656567d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  Plot the target function, the labelled observations (dataset) and the distribution of $x$ values among your dataset. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1996ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "x_plot = np.linspace(min_x,max_x)\n",
    "y_plot = f(x_plot)\n",
    "plt.plot(x_plot, y_plot, \"--\", c=\"slategrey\", label=\"target\")\n",
    "plt.scatter(x, y, marker=\"o\", c=\"tab:blue\", label=\"labelled data\", alpha=0.8)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.hist(x, histtype='bar', ec='black', alpha=0.8)\n",
    "plt.title(\"P(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c4824",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  We will further consider that the relationship between the domain $\\mathcal{X}$ and $\\mathcal{Y}$ can be modelled by a linear model ($\\mathcal{H}$ will be restricted to linear models). <br/>\n",
    "    This may be true or false depending on the function $f$ you've defined. <br/>\n",
    "    Keep in mind that we are not supposed to know $f$, we just postulate that this is true. \n",
    "    <br/>\n",
    "    Our aim will be to find an hypothesis $h \\in \\mathcal{H}$ corresponding to our linear model such as $h \\approx f$.\n",
    "    <br/><br/>\n",
    "    We could for instance consider that the set of hypotheses to consider is $\\mathcal{H} = \\{h | h(x) = \\theta_0 + \\theta_1 x\\}$ <br/>\n",
    "    Finding an hypothesis would therefore correspond to finding the parameters $\\theta = [\\theta_0, \\theta_1]^\\top$, with $\\theta \\in \\mathbb{R}^2$.\n",
    "    <br>\n",
    "    More generally we could consider that our set of hypotheses $\\mathcal{H}$ contains all polynomials of degree $0, 1, \\ldots, k$ such as $\\mathcal{H}_{pol(k)} = \\{h | h(x) = \\sum_{i=0}^{k} \\theta_i x^i\\}$. In this case, finding an hypothesis would therefore correspond to finding the parameters $\\theta = [\\theta_0, \\ldots, \\theta_k]^\\top$, with $\\theta \\in \\mathbb{R}^{k+1}$.\n",
    "    <br/>\n",
    "    Note that defining $\\textbf{x} = [1, x, x^2, \\ldots, x^k]$, we could easily compute $h$ as $h(x) = \\textbf{x}\\theta$.\n",
    "    <br/><br/>\n",
    "    Define a function pol_features that will be used to compute $\\textbf{x} \\in \\mathbb{R}^{k+1}$ for a given $k$ and $x$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d709a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_features(x, degree):\n",
    "    # YOUR TURN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff783c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pol_features(x, degree=2)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f569bbf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Define your error function considering MSE (Mean Squared Error)\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE(\\mathcal{D}, h_\\theta) = \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{\\mathcal{|\\mathcal{D}|}} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{equation*}\n",
    "With $(x^{(i)}, y^{(i)})$ the element $i$ of our dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dedd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    # YOUR TURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.random.rand(nb_samples)\n",
    "error = mse(y_pred, y)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f568b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We will now implement our predictor $h_\\theta$. <br/>\n",
    "This predictor is parametric and fully defined by the associated vector of parameters $\\theta$. <br/>\n",
    "<br/>\n",
    "Create a class LinearRegressor that will be used to instantiate representations of a function $h \\in \\mathcal{H}$.\n",
    "Set the vector of parameters $\\theta$ as an attribute and initialize it randomly.\n",
    "    <br/>\n",
    "In the class, define a function predict that will enable to compute the prediction for a given x. <br/>\n",
    "Next, refine your function to enable the computation of the prediction for several input values in one call. Assume that the polynomial features have been computed beforehand, i.e. considering $h \\in \\mathcal{H}_{pol(k)}$ and $m$ input values, the input given to the predict function will now be $X \\in \\mathbb{R}^{m \\times (k+1)}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor: \n",
    "    def __init__(self, nb_parameters):\n",
    "        ''' nb_parameters is the size of theta'''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X): \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        ''' this will enable to write\n",
    "            h = LinearRegressor(...)\n",
    "            y_pred = h(X) instead of h.predict(X)\n",
    "        '''\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"+\".join([\"\\\\theta_0\"] + [ f\"\\\\theta_{i} x^{i} \" for i in range(1, self.nb_parameters)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df04703",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pol_degree = 4\n",
    "nb_parameters = pol_degree + 1\n",
    "\n",
    "h = LinearRegressor(nb_parameters)\n",
    "X = pol_features(x, pol_degree)\n",
    "\n",
    "y_pred = h(X)\n",
    "\n",
    "error = mse(y, y_pred)\n",
    "\n",
    "print(f\"h(x): {str(h)}\")\n",
    "print(f\"Theta: {h.theta}\")\n",
    "print(f\"MSE : {error:.2f}\")\n",
    "print(f\"RMSE: {np.sqrt(error):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd722db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  Plot the target function, the labelled observations (dataset), and your approximation $h$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, h, pol_features, save_path=None):\n",
    "\n",
    "    X = pol_features(x, h.nb_parameters-1)\n",
    "    y_pred = h(X)\n",
    "    \n",
    "    x_plot = np.linspace(min_x,max_x)[..., np.newaxis]\n",
    "    y_plot = f(x_plot)\n",
    "    \n",
    "    X_plot = pol_features(x_plot, h.nb_parameters-1)\n",
    "    \n",
    "    y_plot_pred = h(X_plot)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.plot(x_plot, y_plot, \"--\", c=\"slategrey\", label=\"target\")\n",
    "    ax.plot(x_plot, y_plot_pred, \"--\", c=\"lightcoral\", label=\"prediction\")\n",
    "    \n",
    "    ax.scatter(x, y, marker=\"o\", c=\"tab:blue\", label=\"labelled data\", alpha=0.8)\n",
    "    ax.scatter(x, y_pred, marker=\"o\", c=\"lightcoral\", label=\"predictions\", alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f\"degree {pol_degree} \\n ${str(h)}$\\n MSE: {mse(y_pred, y)}\")\n",
    "    ax.set_ylim([min_y, max_y])\n",
    "    ax.legend()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    else: \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "plot(x, y, h, pol_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147ed84",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    " We will now define the gradient descent procedure. <br/>\n",
    " The procedure will be iterative:\n",
    "\\begin{equation*}\n",
    "    \\theta^{(t+1)}_j := \\theta^{(t)}_j - \\alpha \\frac{\\partial MSE(\\mathcal{D}, h_\\theta)}{\\partial \\theta_j}\n",
    "\\end{equation*}\n",
    "    \n",
    "First compute the gradient $\\nabla_\\theta MSE(\\mathcal{D}, h_\\theta) = [\\frac{\\partial MSE(\\mathcal{D}, h_\\theta)}{\\partial \\theta_0}, \\ldots]^\\top$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981301e",
   "metadata": {},
   "source": [
    "Gradient of MSE wrt to parameters of a linear regression model\n",
    "\n",
    "Consider $h_\\theta : \\mathbb{R}^{k+1} \\rightarrow \\mathbb{R}$, with $h_\\theta(x) = \\sum_{j = 0}^{k} \\theta_j x_j$.\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE(\\mathcal{D}, h_\\theta) = \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{\\mathcal{|\\mathcal{D}|}} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{equation*}\n",
    "\n",
    "Computing the gradient $\\nabla_\\theta MSE(\\mathcal{D}, h_\\theta)$ requires computing : \n",
    "\\begin{equation*}\n",
    "\\frac{\\partial MSE(\\mathcal{D}, h_\\theta) }{ \\partial\\theta_{j}}\n",
    "\\end{equation*}\n",
    "\n",
    "Considering the chain rule: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial MSE(\\mathcal{D}, h_\\theta) }{ \\partial\\theta_{j}} =\n",
    "\\frac{\\partial MSE(\\mathcal{D}, h_\\theta) }{ \\partial((h_\\theta(x^{(\\cdot)}) - y^{(\\cdot)})^2) }\n",
    "\\frac{ \\partial((h_\\theta(x^{(\\cdot)}) - y^{(\\cdot)})^2) }{\\partial h_\\theta(x^{(\\cdot)}) }\n",
    "\\frac{\\partial h_\\theta(x^{(\\cdot)})}{\\theta_j}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that $MSE(\\mathcal{D}, h_\\theta)$ is:\n",
    "\\begin{equation*}\n",
    "MSE(\\mathcal{D}, h_\\theta) = \\frac{1}{|\\mathcal{D}|} \\sum u(h_\\theta(x^{(i)}), y^{(i)})^2  \n",
    "\\end{equation*}\n",
    "with $u(a,b) = a - b$\n",
    "\n",
    "\\begin{align*}\n",
    " \\frac{\\partial MSE(\\mathcal{D}, h_\\theta)}{ \\partial u} &= \\frac{1}{|\\mathcal{D}|} \\sum 2 u \\\\\n",
    " &= \\frac{2}{|\\mathcal{D}|} \\sum u \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We will now compute $\\frac{\\partial u}{ \\partial h_\\theta(x^{(i)})}$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial u}{ \\partial h_\\theta(x^{(i)})} = \\frac{\\partial (h_\\theta(x^{(i)}) - y^{(i)})}{ \\partial h_\\theta(x^{(i)})} = 1 - 0  = 1 \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "Let's now compute $\\frac{\\partial (h_\\theta(x^{(i)})}{ \\partial \\theta_{j}}$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial (h_\\theta(x^{(i)})}{ \\partial \\theta_{j}} = x^{(i)}_j \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    " \\frac{\\partial MSE(\\mathcal{D}, h_\\theta)}{ \\partial\\theta_{j}} &= \\frac{\\partial MSE(\\mathcal{D}, h_\\theta)}{ \\partial u} \\frac{\\partial u}{ \\partial h_\\theta(x^{(i)})} \n",
    " \\frac{\\partial (h_\\theta(x^{(i)})}{ \\partial \\theta_{j}} \\text{   (chain rule)} \\\\\n",
    " &=  \\frac{2}{|\\mathcal{D}|} \\sum_{i=1}^{\\mathcal{|\\mathcal{D}|}} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot 1 \\cdot x^{(i)}_j \\\\\n",
    "  &=  \\frac{2}{|\\mathcal{D}|} \\sum_{i=1}^{\\mathcal{|\\mathcal{D}|}} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_j \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The gradient can therefore be expressed by: \n",
    "\\begin{equation*}\n",
    "\\nabla_\\theta MSE(\\mathcal{D}, h_\\theta) =  \\frac{2}{|\\mathcal{D}|} \\cdot (\\hat{y}-y)^\\intercal \\cdot X\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdc60b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    " Let's implement our gradient descent algorithms <br/>\n",
    " Define a class OptimizerGD_LinearRegressor implementing the following functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerGD_LinearRegressor():\n",
    "    \n",
    "    def __init__(self, regressor):\n",
    "        ''' regressor is the predictor (h) for which we wante to optimize theta '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def compute_gradient(self, X, y, y_pred):\n",
    "        ''' compute the gradient of the error wrt. to the parameters'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, nb_iterations, learning_rate, X, y, quiet=False):\n",
    "        '''implements gradient descent'''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fe404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit(nb_samples, pol_degree, nb_iterations, learning_rate):\n",
    "    np.random.seed(1234)\n",
    "    x = np.random.uniform(low=min_x, high=max_x, size=(nb_samples,1))\n",
    "    y = f(x) + np.random.randn(nb_samples,1)\n",
    "    \n",
    "   \n",
    "\n",
    "    X = pol_features(x, pol_degree)\n",
    "\n",
    "    h = LinearRegressor(pol_degree + 1)\n",
    "    opt = OptimizerGD_LinearRegressor(h)\n",
    "    opt.fit(nb_iterations, learning_rate, X, y)\n",
    "        \n",
    "    plot(x, y, h, pol_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fit(\n",
    "    nb_samples = 200,\n",
    "    pol_degree = 4,\n",
    "    nb_iterations = 1000000,\n",
    "    learning_rate = 10e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76fda3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    " Use your implementation to optimize your predictor wrt your dataset. <br/>\n",
    " Make sure you're reducing the error at each step.\n",
    " Note that the learning rate will sometime have to be very (very) little.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c900b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sandbox_derivative(): \n",
    "    # Sandbox derivative to detail how the gradient is computed\n",
    "    m  = 10 # nb samples\n",
    "    n = 2  # nb features\n",
    "\n",
    "    X = np.reshape(np.arange(m * n) + 1, (m, n))\n",
    "    theta = np.random.rand(n,1)\n",
    "    y_pred = X @ theta\n",
    "    y = y_pred + 1/2\n",
    "\n",
    "    e = y_pred - y \n",
    "    \n",
    "    def show(label, d):\n",
    "        print(f\"{label}:{d.shape} \\n\\n {d}\\n\")\n",
    "    \n",
    "    show(\"X\", X)\n",
    "    show(\"Theta\",theta)\n",
    "    show(\"e\", e)   \n",
    "    show(\"e.T @ X\", e.T @ X)\n",
    "    show(\"2/m * e.T @ X\", 2/m * e.T @ X)\n",
    "    \n",
    "    \n",
    "sandbox_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac851",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fit(\n",
    "    nb_samples = 100,\n",
    "    pol_degree = 4,\n",
    "    nb_iterations = 10000,\n",
    "    learning_rate = 10e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab5692",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "In this part we will simply generate a video showing the learning steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608dda12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import animation, rc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "img_path = \"/tmp/image_plots\"\n",
    "shutil.rmtree(img_path, ignore_errors=True)\n",
    "Path(img_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb_samples = 100\n",
    "pol_degree = 4\n",
    "nb_steps = 100\n",
    "nb_iterations_step = 10000\n",
    "learning_rate = 10e-7\n",
    "\n",
    "h = LinearRegressor(pol_degree + 1)\n",
    "X = pol_features(x, pol_degree)\n",
    "\n",
    "opt = OptimizerGD_LinearRegressor(h)\n",
    "\n",
    "plot(x, y, h, pol_features, save_path=f\"{img_path}/step_0.jpg\") \n",
    "\n",
    "pbar = tqdm(range(nb_steps), bar_format='{desc}')\n",
    "for i in pbar:\n",
    "    opt.fit(nb_iterations_step, learning_rate, X, y, quiet=True)    \n",
    "    pbar.set_description(f\"step {i+1}/{nb_steps} MSE: {mse(h(X), y):.4f}\")\n",
    "    plot(x, y, h, pol_features, save_path=f\"{img_path}/step_{i+1}.jpg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b20aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(interval=200):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ims = []\n",
    "\n",
    "    print(\"Loading images\")\n",
    "    for i in tqdm(range(nb_steps)):\n",
    "        image = mpimg.imread(f\"{img_path}/step_{i}.jpg\")\n",
    "        im = ax.imshow(image, animated=True)\n",
    "        ims.append([im])\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, ims, interval=interval, blit=False,\n",
    "                                  repeat_delay=1000)\n",
    "    rc('animation', html='html5')\n",
    "    # To save the animation, use\n",
    "    # anim.save(\"movie.mp4\")\n",
    "    plt.close(fig)\n",
    "    print(\"Loading video (may take some time...)\")\n",
    "    return anim\n",
    "\n",
    "anim = show_video()\n",
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
