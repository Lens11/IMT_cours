{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHByHhZxjQy3"
      },
      "source": [
        "<div style=\"text-align:right\"> Practical session 1 <br/> S√©bastien Harispe\n",
        "<h1><center>Keras Introduction: <br/> Practical introduction to Artificial Neural Networks <br/> IMT Mines Al√®s</center></h1>\n",
        "\n",
        "<center>\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/200px-Keras_logo.svg.png)\n",
        "\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiOrjEfFjjvt"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this short tutorial we will see basics of [Keras](https://keras.io/) including: \n",
        "* Generalities about the philosophy and basic constructs of the library.\n",
        "* How to create, train, evaluate, save and use a model.\n",
        "\n",
        "The main sources of information used to prepare this tutorial session are: \n",
        "* https://keras.io\n",
        "* https://www.tensorflow.org\n",
        "* https://www.tensorflow.org/tutorials/keras\n",
        "\n",
        "We will further use ANN to refer to an Artificial Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi2t6ca8j_8S"
      },
      "source": [
        "\n",
        "### Keras\n",
        "\n",
        "**Keras** is a high-level API that can be used to easily build and train ANN such as those used in deep learning models.  \n",
        "It is used for fast prototyping, quick research testing, and production, with three key advantages:\n",
        "* *User friendly* It has a simple, consistent interface optimized for common use cases, and provides clear and actionable feedback for user errors.\n",
        "* *Modular and composable* Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
        "* *Easy to extend* Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n",
        "\n",
        "Keras has originally been developed by Fran√ßois Chollet (Frenchy \\o/) now a Google engineer. \n",
        "\n",
        "Keras is capable of running on top of several plateforms dedicated to neural networks and machine learning. In this practical session we will use the TensorFlow backend. Late developements have made Keras tightly linked to TensorFlow.\n",
        "\n",
        "**TensorFlow** is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google. ‚Äç(‚Äç[source](https://en.wikipedia.org/wiki/TensorFlow))‚Äç.\n",
        "\n",
        "<center>\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/200px-TensorFlowLogo.svg.png)\n",
        "\n",
        "</center>\n",
        "\n",
        "TensorFlow's implementation of the Keras API specification is exposed in the `tf.keras` module. This is a high-level API to build and train models that makes TensorFlow easier to use without sacrificing flexibility and performance.\n",
        "The main competitor of TensorFlow is [PyTorch](https://pytorch.org/) (which do not support Keras API).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYXqPOdnk9af"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "* Basics of machine learning \n",
        "* Basics of ANN (Artificial Neural Networks): you must be familiar with notions such as MLP, Relu, Cross entropy, sofmax, dropout. \n",
        "\n",
        "We provide quick refreshers below. \n",
        "\n",
        "---\n",
        "\n",
        "### Artificial Neuron\n",
        "\n",
        "Artificial neurons are the basic computational units of ANNs.\n",
        "It can generally be viewed as a simple non linear function that will process input values to produce a single output - Note that the term input value does not necessarily refer to the values of the input the predictor must process. Those input values are just numerical values and may be produced by other artificial neurons. Indeed, Artificial neurons are then structured into layers that will form ANNs. \n",
        "\n",
        "In its simple form, an artificial neuron is nothing but an affine function composed with a non linear activation function.  \n",
        "\n",
        "For a given artificial neuron $k$, let there be $m$ input values $x_1$ through $x_m$ and weights $w_{k1}$ through $w_{km}$. We also usually consider an additional input $x_0=1$ to model a bias input with $w_{k0} = b_k$. \n",
        "\n",
        "The output of the $k$-th neuron is: \n",
        "\n",
        "$y_k = \\phi(\\sum_{j=0}^{m} w_{kj} x_j)$\n",
        "\n",
        "$\\phi$ is a defined activation function such as ReLU (see below). \n",
        "\n",
        "---\n",
        "\n",
        "### Rectified Linear Unit (ReLU)\n",
        "\n",
        "ReLU is a popular [activation function](https://en.wikipedia.org/wiki/Activation_function) that can be used while defining an artificial neuron. \n",
        "\n",
        "$\\phi(x) = max(0,x)$\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/500px-Rectifier_and_softplus_functions.svg.png)\n",
        "\n",
        "Using ReLU activation function, the neuron defined above would therefore output: \n",
        "\n",
        "$y_k = max(0, \\sum_{j=0}^{m} w_{kj} x_j)$\n",
        "\n",
        "---\n",
        "\n",
        "### Multi Layer Perceptron (MLP) \n",
        "\n",
        "An MLP is a simple ANN corresponding to one or several layers of neurons.\n",
        "\n",
        "Each neuron of the first layer processes the values of the input to analyse. Neurons of layer $ 1 < l $ will further consider as inputs the outputs of the neurons of layer $l-1$. \n",
        "\n",
        "By stacking layers in such a way we can transform an input vector $x \\in \\mathbb{R}^m$ into an output vector into $\\mathbb{R}^d$. Such an output vector may directly answers the requirement for a regression task, or be used as input of a softmax function to obtain a probability distribution suitable for a classification task, cf. softmax function below.    \n",
        "\n",
        "In MLP, computations that will produce the prediction are made from the input values to the output values without reccurrent connexion between neurons. We say that MLP are feedforward ANN, in opposition for instance to Recurrent ANN (RNN). \n",
        "\n",
        "\n",
        "*Note*: do not be disturbed by the term Perceptron. It refers to a famous old ML algorithm. In the context of ANN, the term perceptron refers to a specific type of artificial neuron based on a specific activation function (heaviside step function). Such artificial neurons are not used anymore in recent ML models, and people use the term MLP to refer to ANN that are composed of layers of any types of artificial neurons. \n",
        "\n",
        "\n",
        "Play with a neural network architecture: https://playground.tensorflow.org/: \n",
        "\n",
        "---\n",
        "\n",
        "### Softmax function \n",
        "\n",
        "The softmax function can be used to transform a vector of real-values $ \\mathbf{z} \\in \\mathbb{R}^{|\\mathcal{C}|}$ into a vector $\\mathbf{z'} \\in [0,1]^{|\\mathcal{C}|}$ such as $\\sum_i^{|\\mathcal{C}|} \\mathbf{z'}_i = 1$, with $\\mathbf{z'}_i = \\text{softmax}(\\mathbf{z})_i$:\n",
        "\n",
        "$\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{|\\mathcal{C}|} e^{z_j}} \\text{ for } i = 1, \\dotsc , |\\mathcal{C}| \\text{ and } \\mathbf z=(z_1,\\dotsc,z_{|\\mathcal{C}|}) \\in \\mathbb{R}^{|\\mathcal{C}|}$\n",
        "\n",
        "The softmax function is often used as the last treatment of a neural network to normalize the output of a network and to obtain a probability distribution over output classes to consider. \n",
        "\n",
        "Considering a muticlass prediction setting with a set of classes $\\mathcal{C}$, the modelling is generally the following:  \n",
        "* A network predict a vector $\\mathbf{z} \\in \\mathbb{R}^{|\\mathcal{C}|}$\n",
        "* $\\mathbf{z}$ is converted using the softmax into a probability distribution $\\mathbf{z'} \\in [0,1]^{|\\mathcal{C}|}$ with $\\sum_i^{|\\mathcal{C}|} \\mathbf{z'}_i = 1$. \n",
        "* The final prediction is then based on a decision rule exploiting $\\mathbf{z'}$, e.g in a single label setting the prediction will be defined as $\\hat{y} := \\text{argmax}_{i \\in 1,\\ldots,|\\mathcal{C}|} \\mathbf{z'}_i$\n",
        "\n",
        "\n",
        "\n",
        "In practice, the softmax can be seen as a soft [argmax](https://en.wikipedia.org/wiki/Arg_max) (differentiable argmax).\n",
        "\n",
        "---\n",
        "\n",
        "### Cross entropy\n",
        "\n",
        "The cross entropy is often used as loss training a ANN for classification.\n",
        "\n",
        "First recall the log loss considered to evaluate a probabilistic binary classifier. \n",
        "\n",
        "For a given input observation we expect a prediction $y \\in \\{0,1\\}$ (label is true or false). \n",
        "The classifier will compute the probability $p \\in [0,1]$ that the input is labelled by the true label: $p = \\hat{\\mathbb{P}}[y=1]$.\n",
        "\n",
        "The loss for this entry can be defined by: \n",
        "\n",
        "$l(p,y) = ‚àí(y log(ùëù)+(1‚àíùë¶)log(1‚àíùëù))$\n",
        "\n",
        "In a *similar fashion*, the cross entropy can be used to consider multiclass settings. It can be used to compare two probability distributions $p$ and $q$ over the same underlying set of events [(source)](https://en.wikipedia.org/wiki/Cross_entropy). Considering two discrete probability distributions $p$ and $q$ over a support $\\mathcal{X}$ the cross entropy $H(p,q) \\in [0,+\\infty]$ is defined by:\n",
        "\n",
        "$H(p,q)=-\\sum _{x\\in {\\mathcal {X}}}p(x)\\,\\log q(x)$. \n",
        "\n",
        "With $\\mathbf{y} \\in [0,1]^{|\\mathcal{C}|}$ the expected distribution and $\\hat{\\mathbf{y}} \\in [0,1]^{|\\mathcal{C}|}$ the predicted one.\n",
        "\n",
        "\n",
        "$H(\\mathbf{y},\\hat{\\mathbf{y}}) = -\\sum _{i\\in {1,\\ldots,|\\mathcal{C}|}} \\mathbf{y}_i\\,\\log \\hat{\\mathbf{y}}_i$.\n",
        "\n",
        "---\n",
        "\n",
        "### Dropout\n",
        "\n",
        "Dropout is a regularization technique for reducing overfitting in ANN by preventing complex co-adaptations on training data (specific case of different regularization strategies, [source](https://en.wikipedia.org/wiki/Dilution_(neural_networks))). It aims at randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network to avoid overfitting on training data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjWiKKEwk5t4"
      },
      "source": [
        "# Keras in Action\n",
        "\n",
        "\n",
        "We will now present some examples and discuss interesting aspects of the API.\n",
        "\n",
        "You'll then be asked to refer to more advanced examples presented in the official documentation : https://www.tensorflow.org/tutorials\n",
        "\n",
        "The version of Tensorflow used in this tutorial is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AKixYBPtiUqZ",
        "outputId": "ac45283b-53a7-4057-c4c2-2f27125f70e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKrctXT0KSi"
      },
      "source": [
        "### Handwritten digit classification\n",
        "\n",
        "Let's consider a first example in which we want to classify images using MNIST dataset.\n",
        "\n",
        "MNIST is a dataset of thousands 28x28 grayscale images representing handwritten digits of the 10 digits. Several examples of images are shown below. \n",
        "\n",
        "https://keras.io/api/datasets/mnist/\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "Providing a grayscale 28x28 image representing an handwritten digit we are interested by predicting the written number. \n",
        "\n",
        "Keep in mind that we are looking for a predictor:\n",
        "\n",
        "$\\hat{f} : [0,255]^{28 \\times 28} \\rightarrow \\mathcal{C} = \\{0,\\ldots, 9\\} \\subset \\mathbb{N}_0$\n",
        "\n",
        "Instead of directly predicting the output class of an input image, we will predict the probability that an input refers to a class. The output of the model will therefore be a probability distribution over $\\mathcal{C}$. \n",
        "\n",
        "Using such a modelling, the cross entropy loss will be used to train our model.\n",
        "\n",
        "We therefore consider :\n",
        "\n",
        "$\\hat{f} : [0,255]^{28 \\times 28} \\rightarrow [0,1]^{|\\mathcal{C}|}$ with for $\\forall x \\in [0,255]^{28 \\times 28}$, $\\hat{f}(x)$ a valid probability distribution.\n",
        "\n",
        "**Code analysis**\n",
        "\n",
        "Keras is an intuitive API.\n",
        "\n",
        "Are you able to understand the source code below?\n",
        "\n",
        "Run it to ensure your installation is working. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-beRhuV3lAw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(f\"size training set {len(x_train)}\")\n",
        "print(f\"size test set {len(x_test)}\")\n",
        "print(f\"dimension {len(x_train[0])} x {len(x_train[0][0])}\")\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 # normalizing dataset\n",
        "\n",
        "\n",
        "mnist_model = Sequential([ \n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(128, activation='relu'), #128 c'est arbitraire car c'est un hyperparam√®tre, on peut changer aussi 'relu'\n",
        "  Dense(10, activation='softmax') \n",
        "])\n",
        "\n",
        "mnist_model.summary()\n",
        "\n",
        "mnist_model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "mnist_model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "print(\"\\nEvaluation\")\n",
        "mnist_model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93vSStK04FYF"
      },
      "source": [
        "<font color='red'>Explained version below : try to understand it by yourself first!</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u7KVkf74GD_"
      },
      "outputs": [],
      "source": [
        "# Explained version\n",
        "# tensorflow.keras.datasets contains several datasets see https://keras.io/api/datasets/\n",
        "from tensorflow.keras.datasets import mnist \n",
        "# Sequential enables plain stack of layers where each layer has exactly one input tensor and one output tensor\n",
        "from tensorflow.keras.models import Sequential \n",
        "# the layers we will use \n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout \n",
        "\n",
        "# Load the dataset retrieving the train and test splits \n",
        "# The dataset will be automatically downloaded from a remote server\n",
        "# In this specific case we are using MNIST\n",
        "# Grayscale 28 x 28 images in a multiclass setting\n",
        "# the dataset already contains train and test splits\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(f\"size training set {len(x_train)}\")\n",
        "print(f\"size test set {len(x_test)}\")\n",
        "print(f\"dimension {len(x_train[0])} x {len(x_train[0][0])}\")\n",
        "print(y_train[0])\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 # normalize grayscale image (max = 255)\n",
        "\n",
        "# We now create our model considering a feedforward network (MLP) consisting of a stacking of layers. \n",
        "# Note that the shape of the input is 28 x 28\n",
        "# The layers will be processed sequentially, using Sequential we pass the layers as parameters \n",
        "# 4 layers are defined :\n",
        "# [1] Flatten, We flatten the 28 x 28 input matrix such as we obtain a vector representation into R^(784=28 x 28).\n",
        "#     The shape of the input is defined for the first layer (dimension of outputs, and internal layer inputs will be infered). \n",
        "# [2] Dense is a fully connected layer parametrized to be composed of 128 neurons each of them considering the ReLU activation function\n",
        "# [3] We apply a dropout between the two last layers (cf. dense layers, layers 2 and 4 ), this is also expressed as a layer.\n",
        "# [4] Last dense layer aims at producing the probability distribution. \n",
        "#     Technically we apply a linear reduction from R^128 to R^10 to which we apply softmax\n",
        "#     to obtain our valid probability distribution\n",
        "mnist_model = Sequential([\n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# The way to process an input is then defined. \n",
        "# Lets look at what looks our network\n",
        "\n",
        "mnist_model.summary()\n",
        "\n",
        "# Model: \"sequential_1\"\n",
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# flatten_1 (Flatten)          (None, 784)               0         \n",
        "# _________________________________________________________________\n",
        "# dense_2 (Dense)              (None, 128)               100480 = 784 x 128 + 128 (biais) \n",
        "# _________________________________________________________________\n",
        "# dropout_1 (Dropout)          (None, 128)               0         \n",
        "# _________________________________________________________________\n",
        "# dense_3 (Dense)              (None, 10)                1290  = 128 x 10 + 10 (biais)     \n",
        "# =================================================================\n",
        "# Total params: 101,770\n",
        "# Trainable params: 101,770\n",
        "# Non-trainable params: 0\n",
        "\n",
        "\n",
        "# We now have to define how to train our model\n",
        "# We will train our model with regard to the cross entropy\n",
        "# using the adam optimizer (that will use the gradient to optimize the parameters)\n",
        "# We also specify some metrics to compute (additionally to the loss)\n",
        "mnist_model.compile(optimizer='adam', #On d√©√©finit le mod√®le\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training will be performed using backpropagation \n",
        "# taking advantage of the gradient computed\n",
        "# from the partial derivative of the error (cross entropy) with regard to the 101,770 parameters\n",
        "# The training set will be used 5 times (5 epochs will be performed)\n",
        "mnist_model.fit(x_train, y_train, epochs=5, batch_size=50) #On entraine le r√©seau de neuronnes\n",
        "\n",
        "print(\"\\nEvaluation\")\n",
        "mnist_model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92JaoKQc6DWp"
      },
      "source": [
        "**Disclaimer** \n",
        "<font color='red'>\n",
        "This example aimed at introducing you to a simple model. This is very far from state-of-the-art models that can are today used to perform image classification. Do not consider to use simple MLP to perform standard image processing (cf. to CNN for instance). Keras advanced tutorial show you how to use state-of-the-art models.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUxvnA8I6glN"
      },
      "source": [
        "## Keras Main notions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9vfn-uM6nMp"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "Keras contains numerous datasets that can be used to test existing or new models on a variety of data types. \n",
        "\n",
        "More information can be found in https://keras.io/datasets/\n",
        "\n",
        "Examples of datasets:\n",
        "* CIFAR10 small image classification. Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n",
        "* CIFAR100 small image classification. Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.\n",
        "* IMDB Movie reviews sentiment classification. Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative).\n",
        "* Reuters newswire topics classification. Dataset of 11,228 newswires from Reuters, labeled over 46 topics.\n",
        "* MNIST database of handwritten digits (0 to 9). Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
        "\n",
        "Datasets can simply be loaded from `tensorflow.keras.datasets`.\n",
        "\n",
        "Those datasets can be very useful for testing existing architectures. \n",
        "\n",
        "**Note**: you can also easily load your dataset from your favorite Numpy structures https://www.tensorflow.org/tutorials/load_data/numpy\n",
        "\n",
        "Considering that your training examples and associated lables are store into Numpy array you can easily do : \n",
        "\n",
        "`train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))`\n",
        "\n",
        "Note however that this is not even required for most use cases dealing with small datasets as you can `fit` a model directly from basic data structures (Numpy arrays, lists) : https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvDdHXcC6oFH"
      },
      "source": [
        "### Models\n",
        "\n",
        "Keras offers various types of models that can be used to define a model defining groups of layers into an object with training and inference features. The various models are defined in `tensorflow.keras.models`. \n",
        "\n",
        "A `Sequential` model can be used to represent a Linear stack of layers. https://keras.io/getting-started/sequential-model-guide/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UVNSjk96ryT"
      },
      "source": [
        "### Layers \n",
        "\n",
        "Layers are used to define a process to apply to an input to produce an output. \n",
        "They will be the building block of the neural nets we will define. \n",
        "In Keras, layers will also be used to apply processing treatments that are not directly defining the architecture of the network, e.g. Dropout.\n",
        "\n",
        "Examples of layers: \n",
        "* Dense: densely connected layers, i.e. each input dimension is linked to the neurons. \n",
        "* Flatten: can be used to flatten a tensor into a vector (1D-Tensor)\n",
        "* Dropout: sets some unit values to 0 (regularization)\n",
        "\n",
        "Layers are explained in https://keras.io/api/layers/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHnRKH36-kP"
      },
      "source": [
        "### Model training and evaluation \n",
        "\n",
        "Finally, the model can be trained and evaluated in a scikit-learn fashion.\n",
        "The traditional steps are: \n",
        "1. Compile\n",
        "2. Fit\n",
        "3. Evaluate\n",
        "4. Predict\n",
        "\n",
        "#### Model Compilation\n",
        "\n",
        "A model can next be compiled by specifying: \n",
        "* The optimizer used to train the network\n",
        "* The loss function to minimize\n",
        "* Metrics that have to be evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gguDmq67DS-"
      },
      "outputs": [],
      "source": [
        "mnist_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26eVJhA47H5G"
      },
      "source": [
        "`tf.keras.Model.compile` takes three important arguments:\n",
        "* `optimizer`: This object specifies the training procedure. Pass it optimizer instances from the `tf.keras.optimizers` module, such as `tf.keras.optimizers.Adam` or `tf.keras.optimizers.SGD`. If you just want to use the default parameters, you can also specify optimizers via strings, such as 'adam' or 'sgd'.\n",
        "* `loss`: The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, and binary_crossentropy. Loss functions are specified by name or by passing a callable object from the `tf.keras.losses module`.\n",
        "* `metrics`: Used to monitor training. These are string names or callables from the `tf.keras.metrics` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0jLyCL87J4q"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUqaypWI7Ogq"
      },
      "outputs": [],
      "source": [
        "mnist_model.fit(x_train, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyzkeDC07TlS"
      },
      "source": [
        "From the documentation you can read \n",
        "\n",
        "`tf.keras.Model.fit` takes three important arguments:\n",
        "* `epochs`: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n",
        "* `batch_size`: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n",
        "* `validation_data`: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument‚Äîa tuple of inputs and labels‚Äîallows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.\n",
        "\n",
        "Below an example using validation data.\n",
        "Note also the interaction with numpy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lwf6w-p7aFO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# parameters\n",
        "size_dataset = 200\n",
        "input_size = 10\n",
        "\n",
        "size_hidden_layer_1 = 64\n",
        "size_hidden_layer_2 = 128\n",
        "\n",
        "# Fixed parameters\n",
        "nb_categories = 3\n",
        "\n",
        "model = tf.keras.Sequential([  \n",
        "  # Adds a densely-connected layer\n",
        "  layers.Dense(size_hidden_layer_1, activation='relu', input_shape=(input_size,)),\n",
        "\n",
        "  # Add another densely-connected layer\n",
        "  layers.Dense(size_hidden_layer_2, activation='relu'),\n",
        "    \n",
        "  # Add a softmax layer with nb_categories output units:\n",
        "  layers.Dense(nb_categories, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Data generating functions\n",
        "def label(x): return 0 if x[0] > 20 else 1 if x[0] > x[-1] else 2\n",
        "def get_labeled_data(size):\n",
        "    d, l = [], []\n",
        "    for x in np.random.randint(100,size=(size, input_size)):\n",
        "        l.append(label(x))\n",
        "        d.append(x)\n",
        "    return np.array(d), tf.keras.utils.to_categorical(np.array(l))\n",
        "    \n",
        "data_train, labels_train = get_labeled_data(10000)\n",
        "data_valid, labels_valid = get_labeled_data(1000)\n",
        "data_test, labels_test = get_labeled_data(1000)\n",
        "\n",
        "model.fit(data_train, labels_train, epochs=20, batch_size=128, validation_data=(data_valid, labels_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKJbT0z27sEU"
      },
      "source": [
        "The tf.keras.Model.evaluate and tf.keras.Model.predict methods can use NumPy data and a tf.data.Dataset.\n",
        "\n",
        "**Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlPOv8nN76pW"
      },
      "outputs": [],
      "source": [
        "result = model.predict(data_test, batch_size=32)\n",
        "\n",
        "for i in range(0,10):\n",
        "    print(f\"data {i}: {data_test[i]}\")\n",
        "    print(f\"probabilities {result[i]} expected: {labels_test[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_6f_QdB8Ssq"
      },
      "source": [
        "**Saving model**\n",
        "\n",
        "The entire model can be saved to a file that contains the weight values, the model's configuration, and even the optimizer's configuration. This allows you to checkpoint a model and resume training later‚Äîfrom the exact same state‚Äîwithout access to the original code.\n",
        "\n",
        "More information about saving and loading a model: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb\n",
        "\n",
        "Note that you can save versions of your model while training it (using callbacks that are introduced below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r7CQWcJi8f3L"
      },
      "outputs": [],
      "source": [
        "# Save entire model to a HDF5 file\n",
        "mnist_model.save('./tmp/mnist_model.h5')\n",
        "\n",
        "# Recreate the exact same model, including weights and optimizer.\n",
        "model = tf.keras.models.load_model('./tmp/mnist_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Msu4kd8kEH"
      },
      "source": [
        "## Additional info\n",
        "\n",
        "**Callbacks**\n",
        "\n",
        "A callback is an object passed to a model to customize and extend its behavior during training. You can write your own custom callback, or use the built-in `tf.keras.callbacks` that include:\n",
        "* `tf.keras.callbacks.ModelCheckpoint`: Save checkpoints of your model at regular intervals.\n",
        "* `tf.keras.callbacks.LearningRateScheduler`: Dynamically change the learning rate.\n",
        "* `tf.keras.callbacks.EarlyStopping`: Interrupt training when validation performance has stopped improving.\n",
        "* `tf.keras.callbacks.TensorBoard`: Monitor the model's behavior using [TensorBoard](https://www.tensorflow.org/tensorboard), a tool that can be used to ease the development of models .\n",
        "\n",
        "To use a `tf.keras.callbacks.Callback`, pass it to the model's fit method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P11IBFWh8ywY"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
        "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='loss'),\n",
        "  # Write TensorBoard logs to `./logs` directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "]\n",
        "\n",
        "\n",
        "mnist_model = Sequential([\n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dropout(0.2),\n",
        "  Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "mnist_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Trains for 5 epochs\n",
        "mnist_model.fit(x_train, y_train, batch_size=50, epochs=5, callbacks=callbacks)\n",
        "\n",
        "print(\"\\nEvaluation\")\n",
        "mnist_model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfkueY8785fB"
      },
      "source": [
        "**Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8h5NVYE89PJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xepiN-Or9Gpc"
      },
      "source": [
        "**Functional syntax**\n",
        "\n",
        "Note that you can use a functional syntax to express your networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "20qNwZXn9M_1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# parameters\n",
        "input_size = 10\n",
        "size_hidden_layer_1 = 64\n",
        "size_hidden_layer_2 = 128\n",
        "\n",
        "# Fixed parameters\n",
        "nb_categories = 3\n",
        "\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(input_size,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(size_hidden_layer_1, activation='relu')(inputs)\n",
        "x = Dense(size_hidden_layer_2, activation='relu')(x)\n",
        "\n",
        "predictions = Dense(nb_categories, activation='softmax')(x)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(data_train, labels_train, epochs=20, batch_size=32, validation_data=(data_valid, labels_valid))\n",
        "\n",
        "result = model.predict(data_test, batch_size=32)\n",
        "print(\"result[0]\",str(result[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op67LhmO9Qsk"
      },
      "source": [
        "You can easily make quick tests using different architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvb4Ras49Vw9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Data generating functions\n",
        "def label(x): return 0 if x[0] > 20 else 1 if x[0] > x[-1] else 2\n",
        "def get_labeled_data(size, input_size):\n",
        "    d, l = [], []\n",
        "    for x in np.random.randint(100,size=(size, input_size)):\n",
        "        l.append(label(x))\n",
        "        d.append(x)\n",
        "    return np.array(d), tf.keras.utils.to_categorical(np.array(l))\n",
        "\n",
        "def addLayers(src_layer, layers_desc):\n",
        "    x = src_layer\n",
        "    for layer_type, size, activation in layers_desc:\n",
        "        layer_class = getattr(layers, layer_type.capitalize())\n",
        "        x = layer_class(size, activation=activation.lower())(x)\n",
        "    return x\n",
        "\n",
        "# parameters\n",
        "input_size = 10\n",
        "size_hidden_layer_1 = 64\n",
        "size_hidden_layer_2 = 128\n",
        "\n",
        "# Fixed parameters\n",
        "nb_categories = 3\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(input_size,))\n",
        "\n",
        "layer_descs = [(\"Dense\", 64, \"relu\"),(\"Dense\", 128, \"relu\"),(\"Dense\", nb_categories, \"softmax\")]\n",
        "layer_descs_xxl = [(\"Dense\", 64, \"relu\")] + ([(\"Dense\", 128, \"relu\"),] * 10) + [(\"Dense\", nb_categories, \"softmax\")]\n",
        "\n",
        "print(layer_descs_xxl)\n",
        "\n",
        "predictions = addLayers(inputs, layer_descs)\n",
        "predictions_xxl = addLayers(inputs, layer_descs_xxl)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_xxl = Model(inputs=inputs, outputs=predictions_xxl)\n",
        "model_xxl.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_xxl.summary()\n",
        "\n",
        "data_train, labels_train = get_labeled_data(10000,input_size)\n",
        "data_valid, labels_valid = get_labeled_data(1000,input_size)\n",
        "data_test, labels_test = get_labeled_data(1000,input_size)\n",
        "\n",
        "model.fit(data_train, labels_train, epochs=200, batch_size=32, validation_data=(data_valid, labels_valid))\n",
        "model_xxl.fit(data_train, labels_train, epochs=200, batch_size=32, validation_data=(data_valid, labels_valid))\n",
        "\n",
        "print(\"*\" * 50)\n",
        "model.evaluate(data_test, labels_test)\n",
        "model_xxl.evaluate(data_test, labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IttUfk1aEiX"
      },
      "source": [
        "# Exercice 1\n",
        "\n",
        "* Download the <a href='https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29'>Breast cancer dataset</a> (or any dataset corresponding to a supervised machine learning problem in which inputs are vectors of numerical values).\n",
        "* Define a neural network that can be tested on this problem. Implement it using Keras.\n",
        "* Compare several networks architectures (modifying the number of layers and specific parameters such as activation functions...).\n",
        "* Discuss the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XIo81KnmiLLb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m load_breast_cancer()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "print(data[\"data\"].shape)\n",
        "print(data[\"data\"][0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[TP 1] Introduction to Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
